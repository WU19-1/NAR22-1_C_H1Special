 /scratch
 etc
 home
 scratch
 usr
 … Network
(DataWarp)	
(Lustre)	
(e.g., qsub, sbatch)
(especially in /, /etc, or /var) that do not confirm to regular
(python library files) with an average of 1,850 methods per
(xtxqtcmd). On a native SLURM system, a simpler prolog
-	42	-	
- DVS Mimics current DSL
- Lustre (packed) Scales with Lustre
- Lustre (unpacked) Scales with Lustre
...) [6], based on glibc or compatible variants, and contain at
/
/home
/var/udi
/var/udiLoop
0	
10	
20	
2x more transactions per second and latencies that were as
30	
40	
50	
60	
70	
<nodes> udiRootSetup
A	
A study by IBM found that the lower overhead of containers
A. Implementation Description
A. Implications
Abstract—Containers are a lightweight virtualization method
Accessing shared resources is also different between the two
Additional complexity
Alva, an Cray XC-30 test and development system. Since it is
An important use-case for the Shifter functionality is to
Another concern is that the images may contain software
App	
App	
App	
App	A’	
App	B	
App	B’	
App	B’	
App	B’	
App	B’	
Approach Pros Cons
At job submission time a prolog runs which calls the udiRoot
At the beginning of job the “setupRoot.sh” script is called
At the conclusion of a Shifter job, the user receives output
At the termination of the job the “unsetupRoot.sh” script is
A’	
B	
B. Security
B. User Experience
Bins/	
Bins/	
Bins/	
Bins/Libs	
Bins/Libs	 App	A	
C. Restrictions on Images Used in Shifter
CHOS - ChrootOS Integrated with the
CHOS NERSC has experience providing similar functionality via CHOS [5]. CHOS was developed at NERSC
CRAY ROOTFS
CRAY ROOTFS Discussions with Cray Engineers led us
CRAY_ROOTFS=UDI
Cached	
Centers like NERSC are increasingly struggling to keep
Compute Node
Compute Node
Compute Node
Compute Node
Considering these different factors, combined with some of
Containers	
Containers promise to offer the flexibility of cloud-type
Cray compute node. The Lustre data point is an unpacked tree
D. Image Gateway
DVS servers can be a
DataWarp. The underlying file system for the Flash storage
Defined Images.
Docker
Docker (MyDock) which allows a user to run specific docker
Docker [1] and its emerging ecosystem, but also includes developments by other players, such as Project Atomic, CoreOS,
Docker daemon on
Docker image repository since we anticipate this will become
Docker in the CLE environment, but rather to automatically
Docker push/pull functionality
Docker was suggested by NERSC staff as an alternative to
Docker.
DockerHub or Private 
DockerHub, as well as to leverage any containers stored in
E. udiRoot – compute node management
EPILOG: launch <nodes> 
Energy Survey to demonstrate how they could use Docker
Environment (DNE), we still expect the Shifter approach will
Even with the advent of Lustre’s Distributed Namespace
Exploiting Linux Containers to support flexible, scalable
F. Workload Manager Integration
Fig. 1. Comparison of VMs versus Docker containers. Each VM requires
Fig. 2. Diagram of the various components of the Shifter prototype for User
Fig. 3. Comparison of Pynamic execution times on different storage options.
Figure 2 shows a basic flow diagram for how the Shifter
Figure 3 shows a plot of the Pynamic benchmark across
File System
File System, Metadata
First	Access	
Flash	
Flash (DataWarp) data point is an unpacked image stored in a
Flash-based file system running on an early version of Cray’s
For example, the biology and genomics community may adopt
From the perspective of the WLM, tearing down the UDI is
From the user’s perspective, it is intended to be relatively
Furthermore, containers have the potential to be more easily
Furthermore, the WLM must provide some mechanism for
GPFS	
GPFS	-	
GPFS file system as a file in GPFS. The final data points is a
GPFS metadata cache.
Gateway server, this does provide an centralized place to
Gateway. At present, there is a simple command line interface
GitHub as way to facilitate the sharing of images and it is
Guest	
Guest	
Guest	
Guest	
Guest	
HPC batch job.
HPC centers. We will also provide performance measurements to
Hardware	
Hardware	
Host	OS	
Host	OS	
Hypervisor	
I. INTRODUCTION
II. BACKGROUND
III. MOTIVATION
IV. IMPLEMENTATION ALTERNATIVES
Image 
Image Gateway
Images (UDI). We will briefly describe some of the options
In Section IV we described some of the potential options
In contrast, a container model can typically map through a
In contrast, full virtual machines typically rely on hardware
In many cases, what users desire is the ability to easily
Interconnect
It is important to use a statically linked sshd from within the
Keywords-Docker; User Defined Images; containers; HPC systems
LVM	over	
LXC [2], OpenVZ) to manage them have existed for nearly a
Libs	
Libs	
Libs	
Linux containers address many of the requirements handled
Linux kernel offers varying levels of isolation depending on
Linux “norms”.
Livermore National Laboratory. This benchmark can be used
Local	Disk	
Local 
Login Node
Lustre
Lustre	 GPFS	-	DVS	 LVM	over	
Lustre file system (the OSTs) and avoid contention on the
Lustre metadata server. This is possible because the metadata
MOM Node
MOM node and the nodehealth check software is engaged
Metadata server could
MyDock - Docker
MyDock Recently, NERSC deveoped a custom wrapper for
NERSC has created an initial implementation of the UDI
Na@ve	
Not designed for user
OS	
OS	
OS	
OS	
OS	
Once the ext4 image file and metadata are generated, imageGateway finally transfers the resulting images to the target
One driving concern in the design of Shifter was security.
One noticeable data point is the GPFS - Native cached performance. GPFS’s client-side cache can perform very well if the
One of the advantages of the Shifter system is that the builtin process management/tracking functionalities of the WLM
One of the primary goals of the Shifter project is to enable
Other Options There are other options we considered but
Owing to the way Shifter is implemented, effectively relying
PROLOG: launch 
Pynamic	Benchmark	
Pynamic is known to stress the metadata performance of a
Pynamic use case). Shifter could be used to create custom
Registry
SLURM [8]. The torque-based setup uses environment variables passed with the job to determine which UDI should be
SUMMARY OF POTENTIAL IMPLEMENTATION APPROACHES.
Shi-er	
Shifter can also facilitate reproducibility. This is particularly
Shifter container since it may be dangerous to run processes
Shifter largely simplifies and streamlines the ability to create
Shifter. A Cray nodehealth check plugin is also included with
Shifter’s approach does a good job of minimizing risks while
Since the images for Shifter are stored and packed on a
Special care is taken to setup etc, var, and opt within /var/udi
Standard Docker enables a user to iteratively modify an
Store
TABLE I
The /var/udi mount point is the target location for setting
The main advantage of containers over virtual machines
The main observation with these results is that the Shifter
The major functions that Shifter needs to perform are:
The use of Linux containers to accelerate development and
The workload manager is a critical component to the
There are several approaches to supporting User Defined
These precautions help to ensure that users cannot subvert the
This also adds complexity to manage and configure the VMs
This helps solve parts of the problem but still requires integration. In addition, there remains the question of where
This is specifically targeted towards allowing users to bring
This issue is particularly noticeable at large scales. This is
This means there is only one instance of the file system client
This symbolic link can point to different directories depending
Three WLM configurations have been used with Shifter:
To achieve all these functions, Shifter is decomposed into
To execute a job on the Shifter-setup compute nodes, the
Torque/Moab with ALPS, SLURM with ALPS, and Native
UDI jobs, it is not possible to load kernel modules for the
UDI setup and teardown is managed by the udiRoot component of Shifter. The udiRoot scripts are typically called by the
Ubuntu as their base OS with a specific version of Perl
User
User Defined Images (UDI).
Uses Docker directly Requires running a
V. PROTOTYPE IMPLEMENTATION
VI. BENCHMARKING AND COMPARISONS
VII. DISCUSSION
VM	
We also conducted other metadata intensive benchmarks
We believe the most critical risk is that a user could elevate
We will briefly describe the different storage options. Shifter
When a connection is initiated to the Image Gateway, xinetd
When submitting a Shifter job, the user has the option to
While Linux has possessed the basic features to support
While Shifter was designed to support User Defined Images,
While at first glance the introduction of User Defined Images
While none of these benchmarks test scaling, in general, we
While we could have used a similar approach here, we were
Wrapper
a cached timing. In most cases, the test were performed on
a defined entry-point and environmental variables, then that
a logical volume accessed through a loop back file. This was
a major security risk if allowed. For security reasons, Shifter
a purpose-built ssh daemon that can be more-or-less safely run
a rich ecosystem for specialized images and applications. For
a simple wrapper script to be written, however.
a test system, it has a small Lustre file system. However, we
a tool to address this challenge [10]. Since images can be
access to the rich ecosystem of available Docker images in
accessing the UDI in the batch job.
actually communicates with a local docker daemon to pull
additional control is provided, Shifter is careful to limit how
all the cgroups mounted in the base OS to enable the WLM
allows Shifter to leverage the more scalable component of the
also ruled out for complexity reasons.
an HPC environment, we elected to only run the docker
an explicit batch script. Some Workload Managers may require
an image that was used in the past. There are limitations
an unpacked image stored in a GPFS file system. Note that
analogous to $PBS NODEFILE) to allow a user to easily
and Python. Meanwhile, the High-Energy Physics community
and average number of methods can be specified during the
and conclusions.
and dependencies and push that image into DockerHub. Next,
and execute malicious code today, so this would be just a new
and exit status just as they would for any other job, and, on the
and integrating Docker into an HPC environment, and some
and interconnect firmware levels) the image may no longer
and invoke environments, but doesn’t provide any capabilities
and issues a command like “docker pull X” where “X”
and others. While this transition is already having an impact
and provide the rationale for the approach we chose.
and removes any kernel modules that were loaded to support
and setuid-incapable, typically on /var/udiLoop. To enable
and technical computing including HPC are still unlcear. We
and to deconstruct it at the end of a job. Workload Manager
and typically shared across containers. This frees up the
and why we believe it has value to the scientific and HPC community. We will then describe our prototype implementation
and “visit” time. In general, the visit time will be similar across
and, of course, tear down the UDI at job completion.
any benchmarks to demonstrate this, but the previously cited
applications. For example, MyDock was used by the Dark
approach
approach performs very well. Cases that must heavily interact with the parallel file system’s metadata service typically
approaches using the Pynamic benchmark [9] from Lawrence
aprun using the CRAY ROOT F S environment variable.
aprun …
aprun. In native SLURM, the plugin automatically chroots the
are accessed natively, there is no additional overhead and we
are emerging to offer similar capabilities, and new layers of
are “nosuid”. Most also use “nodev” except for /dev, of course.
around containers. This adoption is partly being driven by
as features required. First, the script attempts to validate the
as root in an unknown and untrusted environment. The ssh
at an OS environment that is common for their domain but
attempts to detect potentially dangerous path names in regions
audit and scan images for known vulnerabilities. NERSC may
based on a unique identifier that can be used to discriminate
basic provisioning. The overhead to address these requirements
batch and login systems
be based on a relatively “normal” GNU/Linux distribution,
be slow. Finally, native Docker typically performs very well.
be “run”, this means that if the original Docker container has
because Shifter does not try to impose any, but does expose
become a bottleneck
behavior.
believe that container-based computing has the potential to
believe that similar results would be obtained on larger scale
believe that these risks should be weighed carefully and that
between unique versions of an image of the same name (e.g.,
beyond what a regular user already possesses. Where some
bottleneck
bug) it can impact other containers running on that systems.
building a robust, well tested stack with the exact combination
burden of managing other layers of the system (i.e. batch
but isn’t designed to allow users to define their own custom
but to support common application needs, we have constructed
but would apply to other metadata intensive cases. It is also
by full virtual machines (VMs), such as customization and
cache is exceeded, the performance drops off significantly. In
called by the Workload Manager job epilogue. This script is
can be challenging as users have to typically address all of the
can be more effectively cached on the compute node and
can significantly impact subsystem and application performance [3]. According to the study, “In general, Docker equals
can typically install their own software and may not maintain
capabilities such as orchestration are starting to build on the
capability is leveraged by the workload manager integration to
cases, users may be interested in specific tools that are difficult
centers like NERSC. NERSC has termed this capability as
change this risk. Image repositories do introduce a new vector
class system called “Shifter”. We had several goals in mind
cluster or HPC center. For example, the users need to solve
collection of images. Docker is not alone as other competitors
commands and restricts the execution of docker containers to
communicate with the Image Gateway using a simple TCPbased conversational protocol.
communities have turned to the cloud because it promises to
community is planning to add the ability sign and certify
community. In some cases, this can include being able to seamlessly go from their desktop to the HPC environment. Some
complexity and introduces the risk that processes may not get
component contains all the scripts and configurations that
components that would normally be provided by a managed
computational platform. The user does not have any direct
compute node. The final two data points use Docker running
compute nodes, for Shifter we have opted not to directly use
compute nodes.
computing has gained rapid adoption in the past two years.
consequences. One is processes running in containers on a
container based computing. However, this revolution has yet to
container based system only needs to run a single copy of the
container infrastructure.
container.
containers (cgroups and namespaces) and basic tools (i.e.
containers and led to the rapid emergence of an ecosystem
containers can require significantly less memory since a container will often only run the specific application or process
control over this process and the image is rsync’d as a specialpurpose unprivileged user onto the Lustre /scratch filesystem
could alert users that a selected image has a defect and warn
could have been emulated via an iSCSI mount. This was ruled
create a Docker container image with their target application(s)
customization of the image, however, a new rootfs (tmpfs) is
customize and tune the image for use in the HPC environment.
daemon is started chroot’d in /var/udi and functions in many
daemon on a physically discrete system, dubbed the Image
decade, the rapid adoption has been driven by the appearance of a few key technologies that have simplified using
default, docker creates its own cgroups to limit resources and
delays on applications that use dynamically shared libraries.
depending on the implementation. We will discuss this further
designate volume mappings, very similar to the volume mappings supported by regular Docker. This enables a user to map,
device drivers are loaded and the image is mounted readonly
different components based on demand.
disambiguate versions of images to ensure the same version
discover which nodes are part of their job when running in
do not believe these general trends are specific to Pynamic
docker
docker 
does not contain the necessary software to access the HSN. In
done in a similar way as the setup. In the case of ALPS, the
down an image from DockerHub or a private registry, and then
downloaded to the system. Once the image is present, the
dramatically impact scientific computing and we have done
each VM needs to have those services running. Additionally, a
each compute node
early prototyping to investigate how container concepts can be
ease distribution and deployment of applications has recently
ecosystem has emerged around the Docker platform to enable
elevated privileges when running in their defined environment.
enable multi-node calculations to function, even if the UDI
enable parallel filesystem access and install needed configuration files, configure and operate private sshd to enable internode communication, enable user-specified filesystem mappings
entry point can be automatically used without having to write
environment variables up appropriately within the batch script.
environment. Using CHOS on a Cray system would require
environmental translations between the host-system environment and the target UDI. The Image Gateway is responsible
environments, even within a single node. This was driven by
evaluated based on the uncompressed size of the resulting
even across institutions. Docker Hub was largely inspired by
even if libraries for native access to the HSN are not
example of an old theme. In addition, the Docker developer
execute their scientific applications and workflows in the
expect the Shifter approach to scale very well, since metadata
exploded. This revolution is being led by technologies such as
explore how we can leverage this option. For example, we
explore the feasibility and approaches to using Docker on large
ext4 
extract images from native formats and convert to a common
extract the entryepoint, working directory, and environment
extract the image and any needed metadata. The “udiRoot”
features of Docker. In our experience, these restrictions were
features to provide full isolation. Consequently, full VMs
file system from the host system into individual containers.
file system since Python must access each module to build
file systems since the main bottleneck for this benchmark is
first case, the underlying Logical Volume storage resides on a
first terminates the running sshd, if there is one. Next it
for Dynamic Shared Library (DSL ) support and the image
for attackers to exploit. However, users can already download
for determining which nodes are to be used, and thus job
for example having standard canonical paths (/bin, /usr, /etc,
for example, /scratch/user/path to /output within the context of
for how to implement Shifter. Here we compare some of those
for managing the images, keeping a data-store of presently
for non-scheduled interactive access to a UDI image. udiRoot
for running multiple isolated Linux systems under a common
for supporting a small number of managed environments,
format on an external gateway node, and then provide software
four major components: an Image Gateway, command-line
framework for managing container instances coupled with a
function and user interface of Shifter. The workload manager is
function. However, this provides a level of reproducibility that
generated images
generation phase. Once the modules have been generated, a
graph and then extracts the image from it using the “docker
greatly increasing productivity.
has been on cluster-based/CS-series systems, we will describe
has since gained acceptance within the project. MyDock has
have different mechanisms for achieving this. The final area
having a random number of methods. The number of modules
heavily on local disk to function. While it would be possible
host operating system. Container-based computing is revolutionizing the way applications are developed and deployed. A new
how they handle workload management, file systems, and
if the “latest” tag of a Docker container repo changes). This
illustrate the low overhead of containers. While our early work
image
image and then store it using the “docker push” functionality.
image are bind-mounted into /var/udi. If any paths appear
image at this point. All the bind mounts made by setupRoot.sh
image for consumption in a job, customizing the image to
image to be retrieved from DockerHub, or a designated private
image/container providers, and it provides the opportunity to
imageGateway
images as unpacked trees in the Lustre scratch file system and
images for use in Shifter should not make use of path names
images that contain all of the required libraries in the image
images via Docker Hub to improve collaboration and simplify
images which should help address some of the risk.
images. Already scientists are starting to build and package
images. Owing to the complexities of deploying Docker in
impact start up time. Since starting a VM requires initializing a
impacts due to the added overhead. We have not conducted
implications. Furthermore, MyDock requires the Docker daemon to be running on each compute node. This would add
important for scientific work, where scientists may need to
in a Lustre file system and mounted via a loop mount. The
in a global high-performance or use it to share state between
in their own images, but doesn’t allow them to use all of the
in well defined locations. Based on the benchmarking results,
including our rationale for certain design choices and some of
including:
insatiate these environments on large scale HPC systems at
integrate into Docker’s image repository, Docker Hub, this
integrated into HPC environments and benchmarked some of
integrated into traditional HPC environments which means that
integration is critical because the WLM is directly responsible
interested in exploring other alternatives including storing the
interface, and with specific paths mapped into the container.
into system-defined directories during the execution of an
into the ext4 image file. In addition, special care is taken to
is accessed over DVS. The GPFS-Native data point was run
is accessed via Cray’s Data Virtualization Service (DVS).
is actually what puts the user “into” the UDI at job start
is available, imageGateway pulls the image to a local docker
is called on all the nodes separately and in parallel.
is difficult to achieve today. In addition, since Shifter can
is more localized
is not globally shared and each image is read-only mounted.
is ready, the user simply submits a batch job requesting the
is the NERSC implementation which has an Ext4 image stored
is they are typically much more light-weight. For example,
isolation for I/O.
isolation. However, containers rely on capabilities in the
it is currently not feasible to run the native GPFS client on a
it. So this risk exists already and Shifter doesn’t dramatically
iteratively unmounts all the paths under /var/udi (in a reversesorted manner). Finally it unmounts the /var/udiLoop mount
job epilogue will make use of the nodehealth check system
just starting a process so it typically requires fractions of
kernel level to implement these features. This has a few
kernel module that provides a process specific symbolic link.
kernel where as for VMs, each VM has its own instance of the
kernel. These can lead to significant overheads, especially for
key functionality is to get the batch script or other user
large to be feasible. In our experience, what users desire is
launches a python script (imageGateway), which evaluates
less memory and disk space.
levels of isolation for processors and memory, but offer weaker
libraries and start the applications.
likely mode of escalation. Furthermore, privileged services (i.e
linked against LibreSSL and musl (a small, static glibccompatible libc used for micro or embedded Linux systems).
list is written to /var/nodelist within the UDI container (e.g.,
load the modules. The benchmark reports both the import time
loaded images, and transferring images to the computational
locations to support site configurations. Similarly, any siteconfigured parallel filesystems can be bind-mounted into the
makes it easy for collaborators to develop and share images,
makes sense, since full VMs typically require I/O operations
managing needed kernel modules for Shifter, setting up the
mapping. WLM prologue/epilogue functionality is used to
may be possible, the overhead to do the port and validate it
may be too high for a community. Modules [4] can be used to
may conflict with the requirements from another community.
may use Scientific Linux as their platform of choice with
memory that would be required in a VM environment, where
metadata
metadata I/O operations occur at the Lustre object storage
metadata can fit in the limited cache space. However, once this
minimum /bin/sh. The application packaged within the image
models. Most applications need to have access to data stored
module. Most results include both a first access timing and
modules can take minutes to hours on a heavily loaded system
mounted read-write and setuid-incapable, typically on /var/udi.
much as 10x lower for certain transaction rates.
need to be globally maintained (i.e. synchronized).
need to run apache. Other supporting services (i.e. name
node. GPFS-DVS represents the performance of accessing an
nodehealth check fails, the node should be marked admindown
nodes
normal Docker limitations. Among others, the image must
noteworthy that since the parallel file systems and interconnect
of dependencies can be tedious and challenging.
of factors. In some cases, large communities are developing
of megabytes. These extras processes and kernel space also
of our implementation with some of the other implementation
of the application which the user can already control today. So
of the complexity of the requested image. unsetupRoot.sh
of the pitfalls and challenges we encountered. We will discuss
on Linux Containers and Docker, and how they can be of value
on a commodity cluster node with a local disk. These both
on a login node which runs the native GPFS client and used
on a shared system. This is compounded by the fact that the
on chroot and the batch system, there are some minimal
on the enterprise and web space, its implications for scientific
on which process is accessing it. CHOS has worked well
one running Linux kernel operating the host system and all
option the user has is to designate that the container should
options. We will close with some discussion of this prototype
or exceeds KVM performance in every case we tested.” This
or has strict dependencies on the system (i.e. MPI libraries
or other more appropriate image store. The images are named
out due to the added complexity. Another option was to use
overall performance.
overhead. For example, a MySQL benchmark achieved over
pace with the rapid expansion in applications, libraries, and
particular or exclusive use of a UDI. Doing so could represent
path names are detected, the image is rejected. Therefore,
per host system. This can typically be done more securely
perform worse (GPFS - Native, Lustre, GPFS - DVS). DVS
performed to ensure that the latest version of an image is
performs reasonably well after caching, but first access can
platforms. In the case of Docker images, the Image Gateway
poised to become the de facto destination for distributing
port. In some cases, this software may be specifically targeted
porting the kernel module to the Cray OS which may not be
powerful image management system backed by a growing
present in image
primarily due to the overhead for the application to traverse
prior to exec’ing the user process. Different WLM systems
private registries by allowing users to “pull” previously created
problem in our experience, as most applications only make
process chroot’s into /var/udi. This is done in ALPS by
process or user manages to disrupt the kernel (i.e. due to a
prologue and epilogue capabilities of a workload manager, but
prologue and epilogues are used to setup the UDI in the job.
proper location of the filesystem, however different image
properly cleaned up.
proven useful, but it presents several challenges in porting
provide better performance for cases where metadata doesn’t
provide management capabilities. This, however, is at odds
provide this flexibility. However, using a cloud environment
provided to users to login, pull, and list images on the gateway
pull
qsub/
qsub/sbatch/salloc will unblock and the job will be submitted.
reach the HPC community. In this paper, we provide background
registry, and prepare it for use by Shifter. Once the image
relatively light-weight processes that may only require tens
relatively simpler than the setupRoot.sh script and is designed
rely on a purpose-build plugin which adds the “–image” and
represents the tagged container revision. That will cause the
reproducing results. This can help scientists be more productive, but also help them be better scientists.
request. When a new image or a new version of an image
requested image, typically by verifying that it exists in the
requirements that must be true about any image in order
results from IBM support this assertion [3].
return compute nodes to normal state
root executed services) are not run inside the images.
rsync
ruled out early for various reasons. For example, a local disk
run an instance of the file system client. If many VMs are
run on a compute node to actually make the UDI available,
run: as themselves (i.e. non-root), using the native networking
running at scale. This benchmark was designed to measure this
running on each node, this can lead to a dramatic increase in
same environment used for development or adopted by their
saved and tagged with Docker, users can easily bring back
save” functionality. The needed size of the ext4 image is
sbatch
scripting tools. While porting these tools to other OS versions
scripts. On an ALPS-based system, this is performed on the
second. This fast startup can be very useful for highly dynamic
security of the system and escalate their privileges.
security risks. For example, since the processes are all executed as the user (i.e. non-root), the user doesn’t have any
series supercomputers, and a potential partnership with Cray to
services, management, etc) are provided by the host system
setuid and device support disabled. This addresses the most
setup/teardown the Shifter image by calling the appropriate
several different storage options. The setup was generated with
several minutes. In contrast, starting a container is essentially
should not make use of features of Linux kernels newer than
should work natively, even if the WLM uses cgroups. This is
since a variety of custom items need to be placed into these
slurmstepd immediately prior to execing the job script.
so generator.py 495 1850. This corresponds to 495 modules
software to serve their specific scientific community. In other
solely to gain flexibility over the software stack is typical too
some background on container computing including Docker
some benchmarking tests (described later), we determined that
some of the security implications of using Docker and how
some preliminary assessment of supporting Docker on Cray XC
special driver script is executed to measure the time required to
specifying CRAY ROOTFS=UDI in the environment of any
standard Docker installation using a local disk.
steps in a workflow. In a VM environment, each VM must
stored in a Lustre file system. It was executed on a compute
stored. The CRAY ROOT F S directive is typically used
storing the images as loopback mountable image files. After
storing the images in loopback mounted files provided the best
straightforward to use. First, the user needs to either select or
study found that I/O transaction and latency sensitive benchmarks were particularly better performing on Docker. This
submission for either, the equivalent of a “docker pull” is
support different versions of libraries, scripting tools, etc, but
support. NERSC felt it was important to be able to tap into the
supports a number of important capabilities which include
surface does not represent a major departure from any normal
system capable of running efficiently on a Cray XC or XE
system operates and integrates into the workflow of a user.
system run in a common linux kernel. This means that if a
systems and over the interconnect. This is in sharp contrast
systems coupled with the performance of bare-metal systems.
systems, file systems, etc).
systems.
tarball, and then the layers are extracted in the proper order
target (OSTs) level, not in the Lustre metadata target. This
target image. Any aprun/srun will automatically be run within
that capability can be used to prevent introducing additional
that case, the application can still communicate using TCP/IP,
that has (known) flaws. This is largely true already, since users
that is influenced by the WLM is resource management. By
that is needed. For example, a web application may only
that the approach used with Shifter largely mitigates any additional risks. We view the container as basically an extension
the Shifter solution to interoperate between different upstream
the UDI.
the ability to easily define their own images and, then, easily
the alternative implementations. In this paper, we will provide
the batch job, thus enabling /output to be used for writing in
the constraints imposed by the existing architectures. Next, we
the different options, but the import time can vary greatly.
the emergence of technologies like Docker which provide a
the file system to locate the required libraries (similar to the
the full overhead of a OS. Docker containers share a kernel and require much
the image and all mounted file systems are mounted with
the image is an image file (e.g., ext4), then the loop block
the image should reside and in what format it should be
the image, and so some care needs to be taken to set the
the job, supposing of course that “/output” was defined as a
the metadata performance.
the need to support several large HEP projects which had
the number of file system clients the file system must serve.
the one loaded in the base system. In practice this rarely a
the potential deployment issues for running Docker on Cray
the request and then interacts with dockerd to satisfy that
the subsystem. For example, containers can have fairly strong
the use cases that motivate the need for user defined images and
the user has any elevated privileges in the VM environment.
the user logs into the Shifter-enabled computational resource
the uses of Docker. We will describe early work in deploying
their own approved software stacks. CHOS uses a custom
their privileges using something inside the image. However,
them to avoid using the image. However, the authors still
then those paths are skipped and are bind-mounted in /var/udi.
these reason, we focused on options that could integrate with
this approach to the Cray systems. For example, Docker relies
this case, we suspect the entire Pynamic benchmark fit in the
this could significantly reduce the time needed to load the
time this is done by performing a chroot() into /var/udi just
to access the file systems, as well as raises security concerns if
to allow a shared cluster to concurrently support multiple
to be named dangerously (i.e., could confuse the software)
to be robust enough to correctly cleanup the node regardless
to be used within Shifter many of these are similar to
to call the udiRoot scripts on all the relevant compute nodes,
to create a portable execution environment. The choice of
to generate a set of test python modules with each module
to install, have a long list of dependencies, and are difficult to
to investigate options to leverage support in ALPS to “chroot”
to make use of them.
to pack images
to prevent more jobs from scheduling on it.
to remotely call setupRoot.sh on all the compute nodes
to the scientific and HPC community. We will explain some of
to this. For example, if the image has external dependencies
to traverse multiple layers before being serviced which adds
to virtual machine based approaches which typically see large
to work around this requirement, it could have performance
tools demanded by the user community, especially new dataintensive communities. This growth is driven by a number
trivial. But the bigger issue is CHOS’s lack of support for
types have different validation procedures. Second, assuming
typically offer greater protection between applications running
udiRoot scripts on all the relevant compute nodes. The WLM
udiRoot to ensure that nodes are cleaned up properly. If the
udiRootDestroy
under in Section IV.
under some circumstances may also be engaged interactively
unpacked image stored in a GPFS file system via DVS from a
up UDI. All the paths in the base of the loopback-mounted
up the name space. In some cases, the load time for python
use Docker’s thin provisioning Logical Volume driver. In the
use of fairly traditional system calls. Because there is only
used at job run time as was submitted.
used to request a Shifter image and specify options like volume
used, as well as any options. The SLURM implementations
user to interactively manage and select images as well as ease
user-defined images. Finally, CHOS lacks broad community
users can obtain the benefits of flexibility with out the added
using standard Docker commands. All of these commands
using that common image format. This methodology allows
utilities, “udiRoot”, and Workload Manager (WLM) Integration components. The command-line utilities serve to allow a
variables of the final layer to enable a user to later “run” the
verify results years later or duplicate an analysis pipeline but
very specific requirements for certain libraries, compilers, and
virtual machine and booting an entire kernel, booting can take
virtual machine/cloud-style options and the use of Docker
volume target in the original Docker container image. Another
ways similar to Cray’s CCM functionality [7]. Finally, a nodes
we have addressed those for a shared user system typical of
we have since realized it could help address other system related bottlenecks. For example, we have observed startup time
well suited for the ways scientific users need to execute their
when designing Shifter to be an HPC-enabled UDI solution
where Shifter must interact with the image. If any dangerous
whereas in native SLURM the epilogue runs directly on the
which we do not report and found similar results. So we
will present some benchmarks that compare the performance
with a number of options specifying the needed UDI as well
with the traditional model for HPC management of systems.
with updated data. Docker is already being considered as
within Shifter to setup the environment on the compute node(s)
within user-defined containers. This ssh daemon is statically
workloads that may need to quickly shift resources between
would expect to see native performance to the parallel file
would seem to dramatically increase security risks, we believe
“–imagevolume” options to sbatch and salloc. During job
• Ability to fully leverage existing Docker images and
• Accessibility of Shared Resources, Parallel Filesystems,
• Allow user to select or download an image
• Compatibility with Docker as well as other container/image formats
• Compatibility with batch system resource management
• Convert and transfer images
• Integrate UDI request mechanism into Workload Manager
• Mechanism to completely deconstruct any image and
• Mechanism to setup and customize image on compute
• Provide mechanism for generic internode communication,
• Robust, Secure Implementation
• Scalability and Performance of running applications
• Scalability and Performance of setting up the container/image
